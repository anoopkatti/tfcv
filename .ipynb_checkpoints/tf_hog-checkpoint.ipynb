{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "from tf_hog import tf_hog_descriptor\n",
    "\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'viridis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_images(folder):\n",
    "    fnames = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    images = [np.array(Image.open(os.path.join(folder, f))) for f in fnames]\n",
    "    return np.array(images)\n",
    "\n",
    "data = read_images('cifar/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_filters import tf_deriv\n",
    "\n",
    "\n",
    "def tf_select_by_idx(a, idx):\n",
    "    return tf.select(tf.equal(idx, 2), \n",
    "                     a[:,:,:,2], \n",
    "                     tf.select(tf.equal(idx, 1), \n",
    "                               a[:,:,:,1], \n",
    "                               a[:,:,:,0]))\n",
    "\n",
    "\n",
    "def tf_hog_descriptor(images, cell_size = 8, block_size = 2, block_stride = 1, n_bins = 9,\n",
    "                      grayscale = False, oriented = False):\n",
    "\n",
    "    batch_size, height, width, depth = images.shape\n",
    "    half_pi = tf.constant(np.pi/2, name=\"pi_half\")\n",
    "    eps = tf.constant(1e-6, name=\"eps\")\n",
    "    scale_factor = tf.constant(np.pi * n_bins * 0.99999, name=\"scale_factor\")\n",
    "    \n",
    "    img = tf.constant(images, name=\"ImgBatch\", dtype=tf.float32)\n",
    "\n",
    "    # gradients\n",
    "    if grayscale:\n",
    "        gray = tf.image.rgb_to_grayscale(img, name=\"ImgGray\")\n",
    "        grad = tf_deriv(gray)\n",
    "    else:\n",
    "        grad = tf_deriv(img)\n",
    "    g_x = grad[:,:,:,0::2]\n",
    "    g_y = grad[:,:,:,1::2]\n",
    "    \n",
    "    # maximum norm gradient selection\n",
    "    g_norm = tf.sqrt(tf.square(g_x) + tf.square(g_y), \"GradNorm\")\n",
    "    idx    = tf.argmax(g_norm, 3)\n",
    "    \n",
    "    g_norm = tf.expand_dims(tf_select_by_idx(g_norm, idx), -1)\n",
    "    g_x    = tf.expand_dims(tf_select_by_idx(g_x,    idx), -1)\n",
    "    g_y    = tf.expand_dims(tf_select_by_idx(g_y,    idx), -1)\n",
    "\n",
    "    # orientation and binning\n",
    "    if oriented:\n",
    "        # atan2 implementation needed \n",
    "        # lots of conditional indexing required\n",
    "        raise NotImplementedError(\"`oriented` gradient not supported yet\")\n",
    "    else:\n",
    "        g_dir = tf.atan(g_y / (g_x + eps)) + half_pi\n",
    "        g_bin = tf.to_int32(g_dir / scale_factor, name=\"Bins\")  \n",
    "\n",
    "    # cells partitioning\n",
    "    cell_norm = tf.space_to_depth(g_norm, cell_size, name=\"GradCells\")\n",
    "    cell_bins = tf.space_to_depth(g_bin,  cell_size, name=\"BinsCells\")\n",
    "\n",
    "    # cells histograms\n",
    "    hist = list()\n",
    "    zero = tf.zeros(cell_bins.get_shape()) \n",
    "    for i in range(n_bins):\n",
    "        mask = tf.equal(cell_bins, tf.constant(i, name=\"%i\"%i))\n",
    "        hist.append(tf.reduce_sum(tf.select(mask, cell_norm, zero), 3))\n",
    "    hist = tf.transpose(tf.pack(hist), [1,2,3,0], name=\"Hist\")\n",
    "\n",
    "    # blocks partitioning\n",
    "    block_hist = tf.extract_image_patches(hist, \n",
    "                                          ksizes  = [1, block_size, block_size, 1], \n",
    "                                          strides = [1, block_stride, block_stride, 1], \n",
    "                                          rates   = [1, 1, 1, 1], \n",
    "                                          padding = 'VALID',\n",
    "                                          name    = \"BlockHist\")\n",
    "\n",
    "    # block normalization\n",
    "    block_hist = tf.nn.l2_normalize(block_hist, 3, epsilon=1.0)\n",
    "    \n",
    "    # HOG descriptor\n",
    "    hog_descriptor = tf.reshape(block_hist, \n",
    "                                [int(block_hist.get_shape()[0]), \n",
    "                                     int(block_hist.get_shape()[1]) * \\\n",
    "                                     int(block_hist.get_shape()[2]) * \\\n",
    "                                     int(block_hist.get_shape()[3])], \n",
    "                                 name='HOGDescriptor')\n",
    "\n",
    "    return hog_descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3780)\n",
      "Total time: 131.022 sec\n",
      "Session management time: 11.220 sec\n",
      "Evaluation time: 119.803\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "init_op = tf.initialize_all_variables()\n",
    "X = np.repeat(np.repeat(data[:10000], 4, axis=1), 2, axis=2)\n",
    "\n",
    "t1 = time.time()\n",
    "hog = tf_hog_descriptor(X)\n",
    "with tf.Session() as sess:\n",
    "    summary_writer = tf.train.SummaryWriter('tf_logs/', sess.graph)\n",
    "    sess.run(init_op)\n",
    "    t2 = time.time()\n",
    "    g = hog.eval()\n",
    "    print(g.shape)\n",
    "t3 = time.time()\n",
    "\n",
    "print('Total time: %.3f sec' % (t3 - t1))\n",
    "print('Session management time: %.3f sec' % (t2 - t1))\n",
    "print('Evaluation time: %.3f' % (t3 - t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import time\n",
    "\n",
    "k = (1024-64) // 8 * (768-128) // 8\n",
    "X = np.random.randint(0,256, (k,128,64,3), dtype=np.uint8)\n",
    "\n",
    "t1 = time.time()\n",
    "hog = cv.HOGDescriptor()\n",
    "for x in X:\n",
    "    hog.compute(x)\n",
    "t2 = time.time()\n",
    "\n",
    "print('Total time: %.3f sec' % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "\n",
    "def np_hog(a):\n",
    "    n_bins = 9\n",
    "\n",
    "    half_pi = np.pi * 0.5\n",
    "    eps     = 1e-6\n",
    "    scale_factor = n_bins / np.pi * 0.99999\n",
    "\n",
    "    def batch_to_depth(a):    \n",
    "        b = np.transpose(a, [1,2,3,0])\n",
    "        b = np.reshape(b, [b.shape[0], b.shape[1], b.shape[2]*b.shape[3]])\n",
    "        return b\n",
    "\n",
    "\n",
    "    def depth_to_batch(b, batch_size):\n",
    "        c = np.reshape(b, [b.shape[0], b.shape[1], b.shape[2] // batch_size, batch_size])\n",
    "        c = np.transpose(c, [3,0,1,2])\n",
    "        return c\n",
    "\n",
    "    b = batch_to_depth(a)\n",
    "\n",
    "    g_x = np.zeros_like(b)\n",
    "    g_y = np.zeros_like(b)\n",
    "    for i in range(b.shape[2]):\n",
    "        g_x[:,:,i] = convolve2d(b[:,:,i], [[-1,0,1]], mode='same', boundary='symm')\n",
    "        g_y[:,:,i] = convolve2d(b[:,:,i], [[-1],[0],[1]], mode='same', boundary='symm')\n",
    "\n",
    "    g_x = depth_to_batch(g_x, a.shape[0])\n",
    "    g_y = depth_to_batch(g_y, a.shape[0])\n",
    "\n",
    "    g_norm = np.sqrt(np.square(g_x) + np.square(g_y))\n",
    "    idx    = np.argmax(g_norm, axis = -1)\n",
    "    mask   = idx[:,:,:,np.newaxis] == np.arange(a.shape[-1])\n",
    "\n",
    "    g_norm = g_norm[mask].reshape(a.shape[0], a.shape[1], a.shape[2], 1)\n",
    "    g_x    = g_x[mask].reshape(a.shape[0], a.shape[1], a.shape[2], 1)\n",
    "    g_y    = g_y[mask].reshape(a.shape[0], a.shape[1], a.shape[2], 1)\n",
    "\n",
    "    g_dir = np.arctan(g_y / (g_x + eps)) + half_pi\n",
    "    g_bin = np.int32(g_dir * scale_factor)\n",
    "\n",
    "    def extract_patches(a, sizes, strides):\n",
    "        stride_x = strides[0]\n",
    "        stride_y = strides[1]\n",
    "        win_x = sizes[0]\n",
    "        win_y = sizes[1]\n",
    "\n",
    "        size_t = a.dtype.itemsize\n",
    "        batch_n, height_n, width_n, channels_n = a.shape\n",
    "\n",
    "        height_m = (height_n - win_y) // stride_y + 1\n",
    "        width_m  = (width_n  - win_x) // stride_x + 1\n",
    "\n",
    "        strides = ( size_t * channels_n * width_n * height_n, size_t * channels_n * width_n * stride_y,\n",
    "                    size_t * channels_n * stride_x, size_t * channels_n * width_n, size_t * channels_n, size_t )\n",
    "\n",
    "        shape = ( batch_n, height_m, width_n // stride_x, win_y, win_x, channels_n )\n",
    "\n",
    "        b = as_strided(a, strides = strides, shape = shape)[:, :, :width_m, :, :, :]\n",
    "        b = b.reshape(b.shape[0], b.shape[1], b.shape[2], b.shape[3]*b.shape[4]*b.shape[5])\n",
    "        return b\n",
    "\n",
    "    cell_bins = extract_patches(g_bin,  [8, 8], [8, 8])\n",
    "    cell_grad = extract_patches(g_norm, [8, 8], [8, 8])\n",
    "\n",
    "    hist = list()\n",
    "    tmp = np.ma.masked_array(np.zeros_like(cell_bins))\n",
    "    for i in range(n_bins):\n",
    "        mask = cell_bins != i\n",
    "        grad_masked = np.ma.masked_array(cell_grad, mask)\n",
    "        hist.append(np.ma.sum(grad_masked, axis = 3).data)\n",
    "    hist = np.stack(hist, axis = -1)\n",
    "\n",
    "    block_hist = extract_patches(hist, [2,2], [1,1])\n",
    "    k = np.sqrt(np.sum(np.square(block_hist), axis = -1))\n",
    "    k = np.broadcast_to(k, (block_hist.shape[3], k.shape[0], k.shape[1], k.shape[2]))\n",
    "    k = np.transpose(k, [1,2,3,0])\n",
    "\n",
    "    descriptor = np.reshape(block_hist / k, (block_hist.shape[0], \n",
    "                                             block_hist.shape[1]*block_hist.shape[2]*block_hist.shape[3]))\n",
    "    return descriptor"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
